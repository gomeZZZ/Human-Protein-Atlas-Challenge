{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import sys,cv2,gc\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas\n",
    "from Utils.utils import *\n",
    "from ipywidgets import interact\n",
    "import deepdish as dd\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "# set \"context\" (paper, notebook, talk, poster)\n",
    "jtplot.style(theme='grade3',context='talk', fscale=2.5, spines=True, gridlines='-',ticks=True, grid=True, figsize=(6, 4.5))\n",
    "plotcolor = (0, 0.6, 1.0)\n",
    "\n",
    "data_folder = 'D:/data/HPA/all/'\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HDFs for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Collecting garbage...\")\n",
    "gc.collect()\n",
    "\n",
    "#Read Labels\n",
    "label_csv = pandas.read_csv(data_folder+'train.csv')\n",
    "samplecount = label_csv['Id'].size\n",
    "buffer_size = 5000 # How big should the buffers be?\n",
    "idx = 0\n",
    "buffer_idx = 0\n",
    "partNr = 0\n",
    "\n",
    "print(label_csv.head(5))\n",
    "\n",
    "buffer_poi_channel = np.zeros([buffer_size,512,512,1],dtype = np.uint8) #protein of intereset channel only\n",
    "buffer_all_channels = np.zeros([buffer_size,512,512,4],dtype = np.uint8) #all channels\n",
    "labels = np.zeros([buffer_size,28],dtype = np.bool)\n",
    "for _, row in label_csv.iterrows():\n",
    "    \n",
    "    if buffer_idx == buffer_size:\n",
    "        #save\n",
    "        data = {'X': buffer_poi_channel, 'labels': labels}\n",
    "        dd.io.save(data_folder+'poi_'+str(partNr)+'.h5', data,compression=('blosc', 8))\n",
    "        \n",
    "        data = {'X': buffer_all_channels, 'labels': labels}\n",
    "        dd.io.save(data_folder+'all_channel_'+str(partNr)+'.h5', data,compression=('blosc', 8))\n",
    "        data = None #free memory\n",
    "        \n",
    "        if samplecount - idx < buffer_size:\n",
    "            buffer_poi_channel = np.zeros([samplecount - idx,512,512,1],dtype = np.uint8) #protein of intereset channel only\n",
    "            buffer_all_channels = np.zeros([samplecount - idx,512,512,4],dtype = np.uint8) #all channels\n",
    "            labels = np.zeros([samplecount - idx,28],dtype = np.bool)\n",
    "        \n",
    "        #move on\n",
    "        buffer_idx = 0\n",
    "        partNr += 1\n",
    "        \n",
    "    fn = data_folder+'train/'+row['Id']\n",
    "    \n",
    "    if idx % 25 == 0:\n",
    "        printProgressBar (idx, samplecount, prefix = 'Creating HDF5...', suffix = '(' + str(idx) + '/' + str(samplecount) + ')')\n",
    "        \n",
    "    blue,green,red,yellow = cv2.imread(fn+'_blue.png',0),cv2.imread(fn+'_green.png',0),cv2.imread(fn+'_red.png',0),cv2.imread(fn+'_yellow.png',0)\n",
    "    \n",
    "    buffer_poi_channel[buffer_idx] = np.expand_dims(green,axis=2)\n",
    "    buffer_all_channels[buffer_idx,:,:,0] = green\n",
    "    buffer_all_channels[buffer_idx,:,:,1] = red\n",
    "    buffer_all_channels[buffer_idx,:,:,2] = blue\n",
    "    buffer_all_channels[buffer_idx,:,:,3] = yellow\n",
    "    \n",
    "    labelNr = list(map(int,row['Target'].split(' ')))\n",
    "\n",
    "    labels[buffer_idx,labelNr] = True #Convert labels to bool, where entry is true if class is present\n",
    "\n",
    "    if blue is None:\n",
    "        print(\"Error: File not found.\")\n",
    "        \n",
    "    idx += 1\n",
    "    buffer_idx +=1\n",
    "\n",
    "#save last, smaller buffer\n",
    "data = {'X': buffer_poi_channel, 'labels': labels}\n",
    "dd.io.save(data_folder+'poi_'+str(partNr)+'.h5', data,compression=('blosc', 8))\n",
    "\n",
    "data = {'X': buffer_all_channels, 'labels': labels}\n",
    "dd.io.save(data_folder+'all_channel_'+str(partNr)+'.h5', data,compression=('blosc', 8))\n",
    "\n",
    "data = None #free memory\n",
    "buffer_poi_channel = None\n",
    "buffer_all_channels = None\n",
    "\n",
    "print()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HDF5 for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collecting garbage...\")\n",
    "gc.collect()\n",
    "\n",
    "#Read Labels\n",
    "fn_csv = pandas.read_csv(data_folder+'sample_submission.csv')\n",
    "samplecount = fn_csv['Id'].size\n",
    "buffer_size = 11702 # How big should the buffers be?\n",
    "idx = 0\n",
    "\n",
    "print(fn_csv.head(5))\n",
    "\n",
    "poi_channel = np.zeros([buffer_size,512,512,1],dtype = np.uint8) #protein of intereset channel only\n",
    "all_channels = np.zeros([buffer_size,512,512,4],dtype = np.uint8) #all channels\n",
    "\n",
    "for _, row in fn_csv.iterrows():   \n",
    "    fn = data_folder+'test/'+row['Id']\n",
    "    \n",
    "    if idx % 25 == 0:\n",
    "        printProgressBar (idx, samplecount, prefix = 'Creating HDF5...', suffix = '(' + str(idx) + '/' + str(samplecount) + ')')\n",
    "        \n",
    "    blue,green,red,yellow = cv2.imread(fn+'_blue.png',0),cv2.imread(fn+'_green.png',0),cv2.imread(fn+'_red.png',0),cv2.imread(fn+'_yellow.png',0)\n",
    "    \n",
    "    poi_channel[idx] = np.expand_dims(green,axis=2)\n",
    "    all_channels[idx,:,:,0] = green\n",
    "    all_channels[idx,:,:,1] = red\n",
    "    all_channels[idx,:,:,2] = blue\n",
    "    all_channels[idx,:,:,3] = yellow\n",
    "\n",
    "    if blue is None:\n",
    "        print(\"Error: File not found.\")\n",
    "        \n",
    "    idx += 1\n",
    "    \n",
    "#save\n",
    "data = {'X': poi_channel}\n",
    "dd.io.save(data_folder+'test_poi.h5', data,compression=('blosc', 8))\n",
    "\n",
    "data = {'X': all_channels}\n",
    "dd.io.save(data_folder+'test_all_channel.h5', data,compression=('blosc', 8))\n",
    "\n",
    "data = None #free memory\n",
    "poi_channel = None\n",
    "all_channels = None\n",
    "\n",
    "print()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training sets with size 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from skimage import io, transform\n",
    "for partNr in range(7):\n",
    "    print(\"Resizing part \", str(partNr))\n",
    "    gc.collect()\n",
    "    ########################### ONLY POI ###########################\n",
    "    print(\"Resizing \",data_folder+'poi_'+str(partNr)+'.h5')\n",
    "    \n",
    "    d = dd.io.load(data_folder+'poi_'+str(partNr)+'.h5')\n",
    "    \n",
    "    X = d['X'] # torch likes float images\n",
    "    y = d['labels']\n",
    "    X_small = np.zeros((X.shape[0],224,224,1),dtype=np.uint8)\n",
    "\n",
    "    for i,img in enumerate(X):\n",
    "        if i % 25 == 0:\n",
    "            printProgressBar (i, X.shape[0], prefix = 'Resizing images...', suffix = '(' + str(i) + '/' + str(X.shape[0]) + ')')\n",
    "        X_small[i] = np.expand_dims(transform.resize(img.squeeze(), (224, 224), preserve_range=True),axis=2)\n",
    "\n",
    "    print(X_small.shape)\n",
    "    data = {'X': X_small, 'labels': y}\n",
    "    print()\n",
    "    print(\"Writing HDF5...\")\n",
    "    dd.io.save(data_folder+'poi_'+str(partNr)+'_small.h5', data,compression=('blosc', 8))\n",
    "    \n",
    "    ########################### ALL CHANNELS #######################\n",
    "    gc.collect()\n",
    "    print(\"Resizing \",data_folder+'all_channel_'+str(partNr)+'.h5')\n",
    "    \n",
    "    d = dd.io.load(data_folder+'all_channel_'+str(partNr)+'.h5')\n",
    "    \n",
    "    X = d['X'] # torch likes float images\n",
    "    y = d['labels']\n",
    "    X_small = np.zeros((X.shape[0],224,224,4),dtype=np.uint8)\n",
    "\n",
    "    for i,img in enumerate(X):\n",
    "        if i % 25 == 0:\n",
    "            printProgressBar (i, X.shape[0], prefix = 'Resizing images...', suffix = '(' + str(i) + '/' + str(X.shape[0]) + ')')\n",
    "        X_small[i] = transform.resize(img.squeeze(), (224, 224), preserve_range=True)\n",
    "\n",
    "    data = {'X': X_small, 'labels': y}\n",
    "    print()\n",
    "    print(\"Writing HDF5...\")\n",
    "    dd.io.save(data_folder+'all_channel_'+str(partNr)+'_small.h5', data,compression=('blosc', 8))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert test data to 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from skimage import io, transform\n",
    "\n",
    "gc.collect()\n",
    "########################### ONLY POI ###########################\n",
    "print(\"Resizing \",data_folder+'test_poi'+'.h5')\n",
    "\n",
    "d = dd.io.load(data_folder+'test_poi'+'.h5')\n",
    "\n",
    "X = d['X'] # torch likes float images\n",
    "\n",
    "X_small = np.zeros((X.shape[0],224,224,1),dtype=np.uint8)\n",
    "\n",
    "for i,img in enumerate(X):\n",
    "    if i % 25 == 0:\n",
    "        printProgressBar (i, X.shape[0], prefix = 'Resizing images...', suffix = '(' + str(i) + '/' + str(X.shape[0]) + ')')\n",
    "    X_small[i] = np.expand_dims(transform.resize(img.squeeze(), (224, 224), preserve_range=True),axis=2)\n",
    "\n",
    "print(X_small.shape)\n",
    "data = {'X': X_small}\n",
    "print()\n",
    "print(\"Writing HDF5...\")\n",
    "dd.io.save(data_folder+'test_poi'+'_small.h5', data,compression=('blosc', 8))\n",
    "\n",
    "########################### ALL CHANNELS #######################\n",
    "gc.collect()\n",
    "print(\"Resizing \",data_folder+'test_all_channel'+'.h5')\n",
    "\n",
    "d = dd.io.load(data_folder+'test_all_channel'+'.h5')\n",
    "\n",
    "X = d['X'] # torch likes float images\n",
    "\n",
    "X_small = np.zeros((X.shape[0],224,224,4),dtype=np.uint8)\n",
    "\n",
    "for i,img in enumerate(X):\n",
    "    if i % 25 == 0:\n",
    "        printProgressBar (i, X.shape[0], prefix = 'Resizing images...', suffix = '(' + str(i) + '/' + str(X.shape[0]) + ')')\n",
    "    X_small[i] = transform.resize(img.squeeze(), (224, 224), preserve_range=True)\n",
    "\n",
    "data = {'X': X_small}\n",
    "print()\n",
    "print(\"Writing HDF5...\")\n",
    "dd.io.save(data_folder+'test_all_channel'+'_small.h5', data,compression=('blosc', 8))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dd.io.load(data_folder+'poi_0_small.h5')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = d['X']\n",
    "print(X.shape)\n",
    "plt.imshow(X[42].squeeze(),cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
