{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and load torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import sys,cv2\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from Utils.utils import *\n",
    "from Utils.f1_score import *\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "# set \"context\" (paper, notebook, talk, poster)\n",
    "jtplot.style(theme='grade3',context='talk', fscale=2.5, spines=True, gridlines='-',ticks=True, grid=True, figsize=(6, 4.5))\n",
    "plotcolor = (0, 0.6, 1.0)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import deepdish as dd\n",
    "\n",
    "import timeit,gc\n",
    "\n",
    "data_folder = 'D:/data/HPA/all/'\n",
    "model_folder = 'D:/data/HPA/models/'\n",
    "USE_SMALL_IMAGES = True\n",
    "\n",
    "print(\"Using GPU:\",torch.cuda.is_available())\n",
    "print(\"Using device \",torch.cuda.get_device_name(0))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_SMALL_IMAGES:\n",
    "    d = dd.io.load(data_folder+'poi_0_small.h5')\n",
    "else:\n",
    "    d = dd.io.load(data_folder+'poi_0.h5')\n",
    "    \n",
    "X = d['X'].astype(np.float32) / 255.0 # torch likes float images\n",
    "\n",
    "y = d['labels']\n",
    "   \n",
    "print(\"Shapes are:\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize (and load) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_OLD_MODEL = True\n",
    "\n",
    "#Setup resnet model\n",
    "net = models.resnet18(pretrained=False)\n",
    "net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False)\n",
    "if USE_SMALL_IMAGES:\n",
    "    net.fc = nn.Linear(512, 28) #adapt last layer to allow larger input images\n",
    "else:\n",
    "    net.fc = nn.Linear(51200, 28) #adapt last layer to allow larger input images\n",
    "    \n",
    "if LOAD_OLD_MODEL:\n",
    "    if USE_SMALL_IMAGES:\n",
    "        net.load_state_dict(torch.load(model_folder+\"baseline_small.model\"))\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(model_folder+\"baseline.model\")) \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setup BCE Loss and optimizer\n",
    "weights = np.asarray(y.shape[0]/np.sum(y,axis=0))\n",
    "weights[15] = y.shape[0]\n",
    "print(\"Weights = \",weights) # we weight classes given their skewed distribution\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights,dtype=torch.float))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0001)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.collect() \n",
    "\n",
    "epochs = 2\n",
    "batch_size = 100\n",
    "time_per_epoch = 0\n",
    "net.train()\n",
    "\n",
    "iterations_per_epoch = round(30000 / batch_size)\n",
    "runtime = 0\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss,running_f1,actual_idx = 0,0,0\n",
    "    current_buffer_size = X.shape[0]\n",
    "    partNr = 0\n",
    "    \n",
    "    for i in range(iterations_per_epoch):\n",
    "        \n",
    "        if actual_idx*batch_size > y.shape[0]:\n",
    "            print(\"Loading data part \" + str(partNr))\n",
    "            if USE_SMALL_IMAGES:\n",
    "                d = dd.io.load(data_folder+'poi_'+str(partNr)+'_small.h5')\n",
    "            else:\n",
    "                d = dd.io.load(data_folder+'poi_'+str(partNr)+'.h5')\n",
    "                \n",
    "            X = d['X'].astype(np.float32) / 255.0 # torch likes float images\n",
    "            y = d['labels']\n",
    "            actual_idx = 0\n",
    "            partNr += 1\n",
    "            print(\"Done.\")\n",
    "        \n",
    "        start = timeit.default_timer() #measure time\n",
    "        \n",
    "        X_batch = torch.tensor(X[actual_idx:actual_idx+batch_size].transpose(0,3,1,2))\n",
    "        y_batch = torch.tensor(y[actual_idx:actual_idx+batch_size].astype(np.float32),dtype=torch.float)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(X_batch)\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #compute F1 scores\n",
    "        act = torch.sigmoid(outputs)   \n",
    "        label = y_batch.detach().numpy().astype(np.bool)\n",
    "        logits = act.detach().numpy() > 0.5\n",
    "        print(\"Targets in batch = \",np.sum(label),\"Predicted targets = \",np.sum(logits))\n",
    "        running_f1 += f1_score(label,logits)\n",
    "        \n",
    "        #measure runtime\n",
    "        stop = timeit.default_timer()\n",
    "        time_per_epoch = 0.5 * time_per_epoch + 0.5 * (stop-start) * iterations_per_epoch\n",
    "        runtime += (stop-start)\n",
    "        \n",
    "        #print performance metrics\n",
    "        N = ((i+1)*batch_size)\n",
    "        print('[epoch = (%d/%d), iteration = (%3d/%d), time = %3ds, est. time per epoch = %5ds] \\t loss = %.5f ## F1 = %.5f'\\\n",
    "              %(epoch + 1, epochs,i + 1, iterations_per_epoch, runtime, time_per_epoch, running_loss / N, running_f1 / (i+1)))\n",
    "        \n",
    "        actual_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if USE_SMALL_IMAGES:\n",
    "    d = dd.io.load(data_folder+'poi_6_small.h5')\n",
    "else:\n",
    "    d = dd.io.load(data_folder+'poi_6.h5')\n",
    "    \n",
    "Xval = d['X'].astype(np.float32) / 255.0 # torch likes float images\n",
    "yval = d['labels']\n",
    "\n",
    "idx = np.arange(yval.shape[0])\n",
    "\n",
    "np.random.shuffle(idx)\n",
    "Xval = Xval[idx]\n",
    "yval = yval[idx]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run validation\n",
    "gc.collect() \n",
    "net.eval()\n",
    "\n",
    "batch_size = 67\n",
    "iterations_per_epoch = round(Xval.shape[0] / batch_size)\n",
    "time_per_epoch,runtime,running_loss,running_f1 = 0,0,0,0\n",
    "\n",
    "for i in range(iterations_per_epoch):\n",
    "    start = timeit.default_timer() #measure time\n",
    "\n",
    "    X_batch = torch.tensor(Xval[i:i+batch_size].transpose(0,3,1,2))\n",
    "    y_batch = torch.tensor(yval[i:i+batch_size].astype(np.float32),dtype=torch.float)\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(X_batch)\n",
    "\n",
    "    loss = criterion(outputs, y_batch)\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "\n",
    "    #compute F1 scores\n",
    "    act = torch.sigmoid(outputs)\n",
    "\n",
    "    label = y_batch.detach().numpy().astype(np.bool)\n",
    "    logits = act.detach().numpy() > 0.5\n",
    "    print(\"Targets in batch = \",np.sum(label),\"Predicted targets = \",np.sum(logits))\n",
    "    running_f1 += f1_score(label,logits)\n",
    "\n",
    "    #measure runtime\n",
    "    stop = timeit.default_timer()\n",
    "    time_per_epoch = 0.5 * time_per_epoch + 0.5 * (stop-start) * iterations_per_epoch\n",
    "    runtime += (stop-start)\n",
    "    #print performance metrics\n",
    "    N = ((i+1)*batch_size)\n",
    "    print('[iteration = (%3d/%d), time = %3ds, est. time per epoch = %5ds] \\t loss = %.5f ## F1 = %.5f'\\\n",
    "          %(i + 1, iterations_per_epoch, runtime, time_per_epoch, running_loss / N, running_f1 / (i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "if USE_SMALL_IMAGES:\n",
    "    torch.save(net.state_dict(), model_folder+\"baseline_small.model\")\n",
    "else:\n",
    "    torch.save(net.state_dict(),  model_folder+\"baseline.model\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown system (can be run after training and saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cmdCommand = \"shutdown -s\"\n",
    "process = subprocess.Popen(cmdCommand.split(), stdout=subprocess.PIPE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
