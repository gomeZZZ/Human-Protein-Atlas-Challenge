{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and load torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import sys,cv2\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from Utils.utils import *\n",
    "from Utils.f1_score import *\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "# set \"context\" (paper, notebook, talk, poster)\n",
    "jtplot.style(theme='grade3',context='talk', fscale=2.5, spines=True, gridlines='-',ticks=True, grid=True, figsize=(6, 4.5))\n",
    "plotcolor = (0, 0.6, 1.0)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import timeit\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Load training data\n",
    "import deepdish as dd\n",
    "\n",
    "data_folder = 'D:/data/HPA/all/'\n",
    "\n",
    "d = dd.io.load(data_folder+'poi_0.h5')\n",
    "X = d['X'].astype(np.float32) / 255.0 # torch likes float images\n",
    "\n",
    "y = d['labels']\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights =  [3.02571861e-04 2.39234450e-03 9.14913083e-04 2.07900208e-03\n",
      " 1.80505415e-03 1.31926121e-03 2.90697674e-03 1.19474313e-03\n",
      " 8.33333333e-02 5.55555556e-02 8.33333333e-02 3.03030303e-03\n",
      " 4.52488688e-03 6.41025641e-03 2.91545190e-03 1.00000000e+00\n",
      " 5.64971751e-03 1.33333333e-02 3.47222222e-03 2.13675214e-03\n",
      " 1.75438596e-02 8.65051903e-04 3.80228137e-03 1.12485939e-03\n",
      " 1.05263158e-02 4.29922614e-04 1.06382979e-02 1.66666667e-01]\n",
      "Using GPU: True\n",
      "Using device  GeForce GTX 1070\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from Models.simpleConvNet import *\n",
    "#Setup resnet model and optimizer\n",
    "\n",
    "# net = models.resnet18(pretrained=False)\n",
    "# net.fc = nn.Linear(51200, 28) #adapt last layer to allow larger input images\n",
    "# net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False)\n",
    "\n",
    "net = simpleConvNet()\n",
    "\n",
    "#BCE Loss\n",
    "weights = np.asarray(y.shape[0]/np.sum(y,axis=0))\n",
    "weights[15] = y.shape[0]\n",
    "print(\"Weights = \",weights) # we weight classes given their skewed distribution\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights,dtype=torch.float))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.00005)\n",
    "\n",
    "print(\"Using GPU:\",torch.cuda.is_available())\n",
    "print(\"Using device \",torch.cuda.get_device_name(0))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\Models\\simpleConvNet.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.fc1(x)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2ea7e290559e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   1406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "time_per_epoch = 0\n",
    "\n",
    "iterations_per_epoch = round(30000 / batch_size)\n",
    "runtime = 0\n",
    "partNr = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_f1 = 0\n",
    "    \n",
    "    current_buffer_size = X.shape[0]\n",
    "    actual_idx = 0\n",
    "    for i in range(iterations_per_epoch):\n",
    "        if actual_idx*batch_size > y.shape[0]:\n",
    "            print(\"Loading data part \" + str(partNr))\n",
    "            d = dd.io.load(data_folder+'poi_'+str(partNr)+'.h5')\n",
    "            X = d['X'].astype(np.float32) / 255.0 # torch likes float images\n",
    "            y = d['labels']\n",
    "            actual_idx = 0\n",
    "            partNr += 1\n",
    "            print(\"Done.\")\n",
    "        \n",
    "        start = timeit.default_timer() #measure time\n",
    "        \n",
    "        X_batch = torch.tensor(X[actual_idx:actual_idx+batch_size].transpose(0,3,1,2))\n",
    "        y_batch = torch.tensor(y[actual_idx:actual_idx+batch_size].astype(np.float32),dtype=torch.float)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(X_batch)\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #compute F1 scores\n",
    "        act = torch.sigmoid(outputs)\n",
    "        \n",
    "        label = y_batch.detach().numpy().astype(np.bool)\n",
    "        logits = act.detach().numpy() > 0.5\n",
    "        print(\"Targets in batch = \",np.sum(label),\"Predicted targets = \",np.sum(logits))\n",
    "        running_f1 += f1_score(label,logits)\n",
    "        \n",
    "        #measure runtime\n",
    "        stop = timeit.default_timer()\n",
    "        time_per_epoch = 0.5 * time_per_epoch + 0.5 * (stop-start) * iterations_per_epoch\n",
    "        runtime += (stop-start)\n",
    "        #print performance metrics\n",
    "        N = ((i+1)*batch_size)\n",
    "        print('[epoch = (%d/%d), iteration = (%3d/%d), time = %3ds, est. time per epoch = %5ds] \\t loss = %.5f ## F1 = %.5f'\\\n",
    "              %(epoch + 1, epochs,i + 1, iterations_per_epoch, runtime, time_per_epoch, running_loss / N, running_f1 / N))\n",
    "        \n",
    "        actual_idx += 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
